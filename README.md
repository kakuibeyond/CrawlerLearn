# 新闻语料爬取程序说明

## 文件夹说明

- learn：用于学习的文件夹
- source_ly：为钱老师学生ly之前的原始爬虫版本
- news：在上面的基础上，经过一些小改动最终用于爬取新闻数据的爬虫代码
- data/newsdata：爬取的原始数据，标题也视为一段，有重复，无分词
- data/newsdata_w：语料经过去重 分词处理  标题没有去除
- data/M盈利亏损M：对盈利亏损事件做了去重、分词、去标题（因为标题过短 作为一条语料来说语义信息不够 不作为单独的语料单元)，并且按照每20条数据存入一个文档，每行是一条语料

## 爬取方法：典型事件关键词搜索

search_url = 'http://www.nbd.com.cn/'
- 事件：盈利亏损 ['亏转盈','盈转亏','每股盈利','扭亏为盈','营收增长','利润上升','利润下降']
- 事件：报告公告 ['公开转让','财务报表','季度报告','年度报告','临时议案','业绩快报','澄清公告']
- 事件：收购重组 ['放弃收购','资产置换','要约收购','股权收购']
- 事件：上市退市 ['挂牌上市','公开挂牌','终止上市','恢复上市','暂停上市','IPO上市','买壳上市']
- 事件：高管变动 ['高管辞职','高管变动','高管任职']
- 事件：违法纠纷 ['合同纠纷','股权纠纷','借款纠纷','公开谴责','通报批评','监管工作函','重大遗漏','强制摘牌','非法集资']

## 遇到的问题及改进

在爬取数据的过程中，多种因素曾影响我们的数据顺利获取，总结而言包括以下几个方面。

- 新闻文本来源的多样性导致数据获取难度增加，上述提及的新闻网站都有它们的前端设计，在页面切换、元素查找等方面均有所不同，因此要对每一个网站分别设置爬虫算法，分别提取它们对应的文本内容，增大了爬取的难度。
- 部分网页的部分新闻需要订阅/开通会员才允许查看全文，有的直接不允许查看文章，这样会导致我们爬取的文本不完全，影响数据质量。基于此，我们仅从不需要进行额外操作的网页进行了后续新闻的爬取。
- 前期我们在新闻url与文本爬取方面是同步进行，即每找到一个新的新闻网页便直接爬取存储，这种方式效率较为低下，后期我们将其改为先存储所有新闻的url列表，然后统一进行依次爬取，效率有一定提升。
- 由于爬虫程序的响应问题，部分界面可能会重复爬取，同时不同的url指向的也有可能是同一篇新闻报道，因此我们还对原始数据文本进行了去重处理。

## 存的内容

- 根据url获取链接，根据关键词匹配的方法，只存出现了关键词的段落（标题也视为一个段落），每个段落一条语料
- 存储路径示例：.\newsdata\每经网\事件：盈利亏损\亏转盈\1-20.txt
- 每行代表一个语料，即一段出现事件的段落，前后加上'$ '和' $' 分割

## 标注阶段

标注出事件句，事件触发词/关键词，事件中的其他实体（时间，地点，公司）

## 预处理阶段

读取标注结果文档，根据标注的偏移量以及$ $所在的偏移量位置进一步处理，得到所标注实体的偏移位置

## 使用

后期处理其他事件，生成用于标记的原始语料时，将pre_wordseg中的目录分别换成其他事件即可
